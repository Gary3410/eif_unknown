# Unknown Environment EIF
Official implementation of [Embodied Instruction Following in Unknown Environments](https://arxiv.org/abs/2406.11818).

The repository contains:
- The [data](#data-release) used for fine-tuning the model.
- The code for [generating the data](#data-generation-process).
- The code for [fine-tuning the models](#fine-tuning) on RTX 3090 GPUs with [LoRA](https://github.com/microsoft/LoRA).
- The code for [inference](#Running-the-inference).
- The code for [visualization](#visualization).

## ğŸ“ TODO
- [x] Provide inference scripts.
- [ ] Step-by-step initialization tutorial.
- [x] Release pretrained models.
- [ ] Release fine-tuning datasets.
- [ ] Release data-generation scripts.
- [ ] Provide a Dockerfile for installation.

## Setup

### Environmental dependencies
This repository contains the following components:
- Extensive indoor simulation environment [ProcTHOR](https://github.com/allenai/procthor).
- High-level planner and low-level controller [LLaVA](https://github.com/haotian-liu/LLaVA).
- Scene semantic feature extraction [CLIP & OpenCLIP](https://github.com/openai/CLIP).
- Semantic feature map fusion [MinkowskiEngine](https://github.com/NVIDIA/MinkowskiEngine).
- Adaptive weight generation [LongCLIP](https://github.com/beichenzbc/Long-CLIP).
- Open vocabulary instance segmentation model [Detic](https://github.com/facebookresearch/Detic).
- Instance segmentation model [Mask R-CNN](https://github.com/soyeonm/FILM/tree/public/models/segmentation).

### Data preparation
The following data needs to be downloaded:
- ProcTHOR simulator room [layout](https://huggingface.co/Gary3410/eif_unknown/tree/main/procthor_house).
- Fine-tuned [high-level planner](https://huggingface.co/Gary3410/eif_unknown/tree/main/llava-vicuna-v1-3-7b-finetune-planner-lora-high-level-planner) and [low-level controller](https://huggingface.co/Gary3410/eif_unknown/tree/main/llava-vicuna-v1-3-7b-finetune-frontier-lora-low-level-controller).
- Instance segmentation model [weights](https://huggingface.co/Gary3410/eif_unknown/tree/main/Detic_LI_CLIP_SwinB_896b32_4x_ft4x_max-size_procthor) (optional).
- Instruction tuning dataset (optional).

The file directory should be:

```
eif_unknown
â”œâ”€â”€ checkpoints
â”‚   â”œâ”€â”€ bert-large-uncased
â”‚   â”œâ”€â”€ CLIP-ViT-H-14-laion2B-s32B-b79K
â”‚   â”œâ”€â”€ clip-vit-large-patch14
â”‚   â”œâ”€â”€ llava-vicuna-v1-3-7b-finetune-planner-lora-high-level-planner
â”‚   â”œâ”€â”€ llava-vicuna-v1-3-7b-finetune-frontier-lora-low-level-controller
â”‚   â”œâ”€â”€ vicuna-v1-3-7b
â”œâ”€â”€ create_dataset
â”œâ”€â”€ data
â”œâ”€â”€ llava
â”‚   â”œâ”€â”€ eval
â”‚   â”œâ”€â”€ model
â”‚   â”œâ”€â”€ serve
â”‚   â”‚   â”œâ”€â”€ cli_llava_v3_llm_planner.py
â”‚   â”‚   â”œâ”€â”€ cli_llava_v3_procthor_maskrcnn.py
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ train
â”œâ”€â”€ log_file
â”œâ”€â”€ model
â”‚   â”œâ”€â”€ bpe_simple_vocab_16e6.txt.gz
â”‚   â”œâ”€â”€ longclip.py
â”‚   â”œâ”€â”€ model_longclip.py
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ Detic
â”‚   â”‚   â”œâ”€â”€datasets
â”‚   â”‚   â”œâ”€â”€models
â”‚   â”‚   â”‚   â”œâ”€â”€BoxSup-C2_L_CLIP_SwinB_896b32_4x.pth
â”‚   â”‚   â”‚   â”œâ”€â”€BoxSup-C2_Lbase_CLIP_SwinB_896b32_4x.pth
â”‚   â”‚   â”‚   â”œâ”€â”€BoxSup-C2_LCOCO_CLIP_SwinB_896b32_4x.pth
â”‚   â”‚   â”‚   â”œâ”€â”€Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size
â”‚   â”‚   â”‚   â”œâ”€â”€swin_base_patch4_window7_224_22k.pkl
â”‚   â”‚   â”‚   â”œâ”€â”€swin_base_patch4_window7_224_22k.pth
â”‚   â”‚   â”œâ”€â”€ third_party
â”‚   â”‚   â”‚   â”œâ”€â”€ CenterNet2
â”‚   â”‚   â”‚   â”œâ”€â”€ Deformable-DETR
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ segmentation
â”‚   â”‚   â”œâ”€â”€maskrcnn_alfworld
â”‚   â”‚   â”‚   â”œâ”€â”€mrcnn_alfred_objects_008_v3.pth
â”‚   â”‚   â”œâ”€â”€segmentation_helper.py
â”‚   â”‚   â”œâ”€â”€segmentation_helper_procthor.py
â”‚   â”‚   â”œâ”€â”€segmentation_helper_procthor_detic.py
â”œâ”€â”€ docs
â”œâ”€â”€ procthor_house
â”‚   â”œâ”€â”€ test.jsonl.gz
â”‚   â”œâ”€â”€ train.jsonl.gz
â”‚   â”œâ”€â”€ val.jsonl.gz
â”œâ”€â”€ scripts
â”œâ”€â”€ utils
â”œâ”€â”€ visualization
......
```

## ğŸ§ª Evaluation
It is recommended to use at least four 3090 GPUs, or you can evaluate by modifying the configuration with two 3090 GPUs
To evaluate the checkpoint, you can use:

```
# Oracle setting
# Easy task
CUDA_VISIBLE_DEVICES=0,1,2,3 python -m llava.serve.cli_llava_v3_nav_seg_gt \
    --model-path ./checkpoints/llava-vicuna-v1-3-7b-finetune-frontier-lora-low-level-controller \
    --model-path-s1 ./checkpoints/llava-vicuna-v1-3-7b-finetune-planner-lora-high-level-planner \
    --model-base ./checkpoints/vicuna-v1-3-7b \
    --image-file ./vision_dataset/llava_dataset_v8_easy_train/frontiers_feature \
    --val-file ./data/spaced_parse_instruction_easy_v12_val.json
```
```
# Detic setting
# Easy task
CUDA_VISIBLE_DEVICES=0,1,2,3 python -m llava.serve.cli_llava_v3_procthor_maskrcnn \
    --model-path ./checkpoints_output/llava-vicuna-v1-3-7b-finetune-frontier-lora-low-level-controller \
    --model-path-s1 ./checkpoints_output/llava-vicuna-v1-3-7b-finetune-planner-lora-high-level-planner \
    --model-base ./checkpoints/vicuna-v1-3-7b \
    --image-file ./vision_dataset/llava_dataset_v8_easy_train/frontiers_feature \
    --val-file ./data/spaced_parse_instruction_easy_v12_val.json
```

# ğŸ·ï¸ License
This repository is released under the MIT license.

# ğŸ¥° Citation
If you find this repository helpful, please consider citing:

```
@article{wu2024embodied,
  title={Embodied instruction following in unknown environments},
  author={Wu, Zhenyu and Wang, Ziwei and Xu, Xiuwei and Lu, Jiwen and Yan, Haibin},
  journal={arXiv preprint arXiv:2406.11818},
  year={2024}
}
```
